import pynever.strategies.bounds_propagation.utility.functions as utilf
from pynever import networks, nodes, tensors
from pynever.strategies.abstraction.star import ExtendedStar
from pynever.strategies.bounds_propagation.bounds import VerboseBounds
from pynever.strategies.bounds_propagation.linearfunctions import LinearFunctions


def abs_propagation(star: ExtendedStar, network: networks.SequentialNetwork, bounds: VerboseBounds) -> ExtendedStar:
    """
    This method performs the abstract propagation of a single star starting
    from a specific layer and neuron. The output is a single star that uses
    approximation in the next layers

    Parameters
    ----------
    star : ExtendedStar
        The star to process
    network : networks.SequentialNetwork
        The neural network to propagate through
    bounds : VerboseBounds
        The bounds of the network layers

    Returns
    ----------
    ExtendedStar
        The resulting star approximate with the abstract propagation

    """

    if star.ref_layer is None:
        return star

    start_index = network.get_index_from_id(star.ref_layer)
    for layer in network.layers_iterator(start_index):

        # Propagate fully connected entirely
        if isinstance(layer, nodes.FullyConnectedNode):
            # Need to expand bias since they are memorized like one-dimensional vectors in FC nodes.
            bias = layer.get_layer_bias_as_two_dimensional()
            star = star.single_fc_forward(layer.weight, bias)

        elif isinstance(layer, nodes.ConvNode):
            # The star generated by the convolution is interpreted as the linearization
            # provided by symbolic propagation
            star = make_star_from_bounds(bounds, layer.identifier)

        # Propagate ReLU starting from target
        elif isinstance(layer, nodes.ReLUNode):
            star = star.approx_relu_forward(bounds, layer.identifier)

        elif isinstance(layer, nodes.FlattenNode):
            # Do nothing
            continue

        # =======================================
        # There is a network with two useless
        # Reshape layers that do nothing, here
        # we filter them when this occurs
        # =======================================
        elif ((isinstance(layer, nodes.ReshapeNode) and isinstance(network.get_next_node(layer), nodes.ReshapeNode)) or
              (isinstance(layer, nodes.ReshapeNode) and isinstance(network.get_prev_node(layer), nodes.ReshapeNode))):
            # Do nothing
            continue

        else:
            raise NotImplementedError(f'Unsupported layer {layer.__class__}')

    return star


def propagate_and_init_star_before_relu_layer(star: ExtendedStar, bounds: VerboseBounds,
                                              network: networks.SequentialNetwork, skip: bool = True) -> ExtendedStar:
    """
    Compute the initial star which will always start from the first layer and
    where we will use the bounds to determine the inactive nodes,
    so that we could set the transformation for them to 0.

    """

    new_star, relu_layer = propagate_until_relu(star, bounds, network, skip=skip)
    relu_layer_id = new_star.ref_layer

    if relu_layer is not None:
        layer_inactive = utilf.compute_layer_inactive_from_bounds_and_fixed_neurons(bounds, new_star.fixed_neurons,
                                                                                    relu_layer_id)

        new_transformation = new_star.mask_for_inactive_neurons(layer_inactive)

        return ExtendedStar(new_star.get_predicate_equation(), new_transformation, ref_layer=relu_layer_id,
                            fixed_neurons=new_star.fixed_neurons, enforced_constraints=star.enforced_constraints,
                            input_differences=star.input_differences)

    return new_star


def propagate_until_relu(star: ExtendedStar, bounds: VerboseBounds, network: networks.SequentialNetwork, skip: bool) \
        -> tuple[ExtendedStar, nodes.ReLUNode | None]:
    """
    This function performs the star propagation throughout Fully Connected layers
    only, until a ReLU layer is encountered. This is used in order to process
    Fully Connected layers only once per cycle

    Parameters
    ----------
    star : ExtendedStar
        The star to process
    bounds : VerboseBounds
        The bounds collection
    network : networks.SequentialNetwork
        The neural network
    skip : bool
        Flag to signal end of propagation

    Returns
    ----------
    tuple[ExtendedStar, nodes.ReLUNode]
        The resulting star before the next ReLU layer and the ReLU layer

    """

    relu_layer = None
    for layer in network.layers_iterator():
        if skip:
            if layer.identifier == star.ref_layer:
                skip = False

        else:
            # Propagate fully connected entirely
            if isinstance(layer, nodes.FullyConnectedNode):
                # Need to expand bias since they are memorized like one-dimensional vectors in FC nodes.
                bias = layer.get_layer_bias_as_two_dimensional()
                star = star.single_fc_forward(layer.weight, bias)

            elif isinstance(layer, nodes.ConvNode):
                # The star generated by the convolution is interpreted as the linearization
                # provided by symbolic propagation
                star = make_star_from_bounds(bounds, layer.identifier)

            elif isinstance(layer, nodes.ReLUNode):
                relu_layer = layer
                break

            elif isinstance(layer, nodes.FlattenNode):
                # Do nothing
                pass

            # =======================================
            # There is a network with two useless
            # Reshape layers that do nothing, here
            # we filter them when this occurs
            # =======================================
            elif ((isinstance(layer, nodes.ReshapeNode) and isinstance(network.get_next_node(layer),
                                                                       nodes.ReshapeNode)) or
                  (isinstance(layer, nodes.ReshapeNode) and isinstance(network.get_prev_node(layer),
                                                                       nodes.ReshapeNode))):
                # Do nothing
                pass

            else:
                raise NotImplementedError(f'Unsupported layer {layer.__class__}')

    # Set reference layer
    if relu_layer is not None:
        star.ref_layer = relu_layer.identifier
    else:
        # No ReLU layer means we are at the end
        star.ref_layer = network.get_last_node().identifier

    return star, relu_layer


def make_star_from_bounds(bounds: VerboseBounds, layer_id: str) -> ExtendedStar:
    """
    This function creates an ExtendedStar from the symbolic equations of the bounds at the
    layer specified by layer_id

    Parameters
    ----------
    bounds : VerboseBounds
        The collection of the symbolic and concrete bounds
    layer_id : str
        The identifier of the current layer

    Returns
    ----------
    ExtendedStar
        The ExtendedStar with the constraints specified by the bounds

    """

    symbolic_bounds = bounds.symbolic_bounds[layer_id]
    numeric_bounds = bounds.numeric_post_bounds[layer_id]

    layer_size = symbolic_bounds.lower.matrix.shape[0]
    input_size = symbolic_bounds.lower.matrix.shape[1]

    predicate_matrix = []
    predicate_bias = []

    # Build the predicate in normal form Cx <= d
    for i in range(layer_size):
        predicate_matrix.append(-symbolic_bounds.lower.matrix[i])
        predicate_matrix.append(symbolic_bounds.upper.matrix[i])

        predicate_bias.append(-symbolic_bounds.lower.offset[i] - numeric_bounds.lower[i])
        predicate_bias.append(symbolic_bounds.upper.offset[i] + numeric_bounds.upper[i])

    predicate = LinearFunctions(tensors.array(predicate_matrix), tensors.array(predicate_bias))
    identity_transformation = LinearFunctions(tensors.identity(input_size), tensors.zeros((input_size, 1)))

    return ExtendedStar(predicate, identity_transformation, ref_layer=layer_id)
